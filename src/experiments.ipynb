{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All imports for each of our downstream cells.\n",
    "\"\"\"\n",
    "\n",
    "# Simulator\n",
    "from mpm.engine import Engine\n",
    "from mpm.cube import cube\n",
    "\n",
    "# Utils\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Final, List, Tuple, Any\n",
    "\n",
    "# ML\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, History\n",
    "from tensorflow.keras.losses import Huber\n",
    "import pickle\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Comment this out if printing causes crashes.\n",
    "np.set_printoptions(suppress=True, threshold=np.inf, linewidth=500, precision=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some helper constants for the training data\n",
    "\"\"\"\n",
    "\n",
    "# Number of simulation steps\n",
    "_TRAIN_DATA_STEPS = 2000\n",
    "\n",
    "_TRAIN_DATA_GMIN: Final[int] = -300\n",
    "_TRAIN_DATA_GMAX: Final[int] = -10\n",
    "\n",
    "# The gravitation increment for the sim\n",
    "_TRAIN_DATA_GINCR: Final[int] = 10\n",
    "\n",
    "# Resolutions for the paper - These are doubled due to there being two cubes\n",
    "_TRAIN_DATA_RESOLUTIONS: Final[List[int]] = [200, 400, 600]\n",
    "\n",
    "# Coarseness for the paper - The sampling frequency\n",
    "_TRAIN_DATA_COARSENINGS: Final[List[int]] = [10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some helpter constants for the test data\n",
    "\"\"\"\n",
    "# Number of simulation steps\n",
    "_TEST_DATA_STEPS = 3000\n",
    "\n",
    "# We only run for this gravitation scenario\n",
    "_TEST_DATA_GMIN: Final[int] = -100\n",
    "_TEST_DATA_GMAX: Final[int] = -100\n",
    "\n",
    "# Static coarsening, we use this to not surprise the model _too_ much\n",
    "_TEST_DATA_COARSENING: Final[int] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Some helpter constants for ML models \n",
    "\"\"\"\n",
    "\n",
    "# If you're in a 3D simulation, this value is 4\n",
    "_CHANNELS: Final[int] = 3\n",
    "\n",
    "# The shape of the input to the neural network\n",
    "_INPUT_SHAPE: Final[Tuple[int, int, int]] = (64, 64, _CHANNELS)\n",
    "\n",
    "# Determined in the paper, your mileage may vary.\n",
    "_LR: Final[float] = 0.0001\n",
    "\n",
    "# Vary this value to use a different loss \"mae\", \"mse\", \"huber\" etc.\n",
    "_LOSS: Final[str] = \"huber\"\n",
    "\n",
    "# Exponent value to control the size of the model [Taken directly from Thuerey et. al.]\n",
    "# 3 - 86k\n",
    "# 4 - 330k\n",
    "# 5 - 1.3m\n",
    "# 6 - 5.4m\n",
    "# 7 - 21m\n",
    "_EXPO: Final[int] = 3\n",
    "\n",
    "# Our dropout perccentage\n",
    "_DROPOUT: Final[float] = 0.2\n",
    "\n",
    "# The number of training epochs\n",
    "_EPOCHS: Final[int] = 200\n",
    "\n",
    "# Batch size, you may need to change this depending on available gpu memory\n",
    "_BATCH_SIZE: Final[int] = 25\n",
    "\n",
    "# Validation Split size\n",
    "_VALIDATION_SPLIT: Final[float] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some helper constants for plots\n",
    "\"\"\"\n",
    "\n",
    "# The y axis labels for the ground truth plots\n",
    "_Y_AXIS_LABELS_GROUND_TRUTH = [\"X-Velocity\", \"Y-Velocity\", \"Mask\"]\n",
    "\n",
    "# The output color for the matplotlib plots\n",
    "_CMAP: Final[str] = \"plasm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate Datasets\n",
    "\"\"\"\n",
    "\n",
    "# Make the training-set cubes in the simulation\n",
    "def make_train_sim_object(res: int, material_model: str):\n",
    "    center = (0.4, 0.6)\n",
    "    x1 = cube(center=center, res=res)\n",
    "\n",
    "    center = (0.3, 0.5)\n",
    "    x2 = cube(center=center, res=res)\n",
    "\n",
    "    # Move this cube closer to the ground.\n",
    "    x2[:, 1] -= 0.2\n",
    "\n",
    "    # Combine the points into one large matrix of points\n",
    "    cubes = np.concatenate((x1, x2))\n",
    "\n",
    "    # Set material type\n",
    "    material = np.array([material_model] * len(cubes))\n",
    "\n",
    "    return cubes, material\n",
    "\n",
    "\n",
    "def generate_all_train_datasets():\n",
    "    e = Engine()\n",
    "    for r in _TRAIN_DATA_RESOLUTIONS:\n",
    "        for c in _TRAIN_DATA_COARSENINGS:\n",
    "            for model in (\"jelly\", \"liquid\", \"snow\"):\n",
    "                x, material = make_train_sim_object(r, model)\n",
    "                e.generate(\n",
    "                    x,\n",
    "                    material,\n",
    "                    steps=_TRAIN_DATA_STEPS,\n",
    "                    gmin=_TRAIN_DATA_GMIN,\n",
    "                    gmax=_TRAIN_DATA_GMAX,\n",
    "                    incr=_TRAIN_DATA_GINCR,\n",
    "                    coarsening=c,\n",
    "                )\n",
    "\n",
    "\n",
    "# Make the testing-test cubes in the simulation\n",
    "def make_test_sim_object(res: int):\n",
    "    bounds1 = (0.4, 0.6)\n",
    "    x1 = cube(bounds1, res=res)\n",
    "    x1[:, 1] += 0.25\n",
    "    material1 = np.array([\"jelly\"] * len(x1))\n",
    "\n",
    "    bounds2 = (0.4, 0.6)\n",
    "    x2 = cube(bounds2, res=res)\n",
    "    material2 = np.array([\"snow\"] * len(x2))\n",
    "\n",
    "    bounds3 = (0.4, 0.6)\n",
    "    x3 = cube(bounds3, res=res)\n",
    "    x3[:, 1] -= 0.25\n",
    "    material3 = np.array([\"liquid\"] * len(x3))\n",
    "\n",
    "    cubes = np.concatenate((x1, x2, x3))\n",
    "    materials = np.concatenate((material1, material2, material3))\n",
    "\n",
    "    return cubes, materials\n",
    "\n",
    "\n",
    "def generate_all_test_datasets():\n",
    "    e = Engine()\n",
    "    resolution = 500\n",
    "    x, material = make_test_sim_object(resolution)\n",
    "    e.generate(\n",
    "        x,\n",
    "        material,\n",
    "        steps=_TEST_DATA_STEPS,\n",
    "        gmin=_TEST_DATA_GMIN,\n",
    "        gmax=_TEST_DATA_GMAX,\n",
    "        coarsening=_TEST_DATA_COARSENING,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load datasets and pre-process them\n",
    "\"\"\"\n",
    "\n",
    "def select_and_load_dataset(r: int, c: int):\n",
    "    files = list(filter(lambda x: x.endswith(\".npz\"), os.listdir(\"old_datasets/\")))\n",
    "    files = list(filter(lambda f: f\"res_{r}\" in f and f\"coarse_{c}\" in f, files))\n",
    "    files = list(map(lambda x: f\"old_datasets/{x}\", files))\n",
    "    jelly = [f for f in files if \"jelly\" in f][0]\n",
    "    snow = [f for f in files if \"snow\" in f][0]\n",
    "    liquid = [f for f in files if \"liquid\" in f][0]\n",
    "\n",
    "    return np.load(jelly), np.load(snow), np.load(liquid)\n",
    "\n",
    "def make_inputs_and_targets(ds: np.ndarray):\n",
    "    m_ini = (ds[\"initial\"][:, :, :, 0] != 0) | (ds[\"initial\"][:, :, :, 1] != 0)\n",
    "    m_fin = (ds[\"final\"][:, :, :, 0] != 0) | (ds[\"final\"][:, :, :, 1] != 0)\n",
    "    m_ini = np.expand_dims(m_ini, axis=3)\n",
    "    m_fin = np.expand_dims(m_fin, axis=3)\n",
    "    return [\n",
    "        np.concatenate((ds[\"initial\"], m_ini), axis=3),\n",
    "        np.concatenate((ds[\"final\"], m_fin), axis=3),\n",
    "    ]\n",
    "\n",
    "def transform(ds: np.ndarray):\n",
    "    ds[0] /= 140\n",
    "    ds[1] /= 140\n",
    "    ds[0][:, :, :, 2] *= 140\n",
    "    ds[1][:, :, :, 2] *= 140\n",
    "\n",
    "def load_train_datasets_by_res_and_coarseness(res: int, coarseness: int):\n",
    "    dataset_jelly, dataset_snow, dataset_liquid = select_and_load_dataset(res, coarseness)\n",
    "    dataset_jelly = make_inputs_and_targets(dataset_jelly)\n",
    "    dataset_snow = make_inputs_and_targets(dataset_snow)\n",
    "    dataset_liquid = make_inputs_and_targets(dataset_liquid)\n",
    "\n",
    "    transform(dataset_jelly)\n",
    "    transform(dataset_snow)\n",
    "    transform(dataset_liquid)\n",
    "\n",
    "    return dataset_jelly, dataset_snow, dataset_liquid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Training\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def infer_model_name(expo):\n",
    "    size = \"86k\"\n",
    "    if expo == 4:\n",
    "        size = \"330k\"\n",
    "    elif expo == 5:\n",
    "        size = \"1_3m\"\n",
    "    elif expo == 6:\n",
    "        size = \"5_4m\"\n",
    "    elif expo == 7:\n",
    "        size = \"21m\"\n",
    "    return size\n",
    "\n",
    "\n",
    "def train_model(X, y, name=\"\", expo=_EXPO, dropout=_DROPOUT, lr=_LR, save=True):\n",
    "    early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    nn = make_model(expo=expo, dropout=dropout, lr=lr)\n",
    "    print(\"Neural network parameters: \", nn.count_params())\n",
    "    history = nn.fit(\n",
    "        X,\n",
    "        y,\n",
    "        epochs=_EPOCHS,\n",
    "        batch_size=_BATCH_SIZE,\n",
    "        validation_split=_VALIDATION_SPLIT,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    if save:\n",
    "        if name == \"\":\n",
    "            name = f\"network_{infer_model_name(expo)}_lr_{lr}\"\n",
    "        print(f\"Saving neural network to {name}.h5\")\n",
    "        nn.save(f\"{name}.h5\")\n",
    "\n",
    "        print(f\"Saving history to history_{name}.pickle\")\n",
    "        with open(f\"history_{name}.pickle\", \"wb+\") as f:\n",
    "            pickle.dump(history, f)\n",
    "    return history\n",
    "\n",
    "def train_all_models_by_size():\n",
    "    for expo in (3, 4, 5, 6, 7):\n",
    "        for r in (400, 800, 1200):\n",
    "            for c in (10, 15, 20):\n",
    "                j, s, l = load_train_datasets_by_res_and_coarseness(r, c)\n",
    "                X = np.concatenate((j[0], s[0], l[0]))\n",
    "                y = np.concatenate((j[1], s[1], l[1]))\n",
    "                name = f\"network_{infer_model_name(expo)}_res_{r}_coarse_{c}_lr_{_LR}\"\n",
    "                train_model(X, y, name, expo=expo, dropout=_DROPOUT, lr=_LR, save=True)\n",
    "\n",
    "def train_all_models_by_learning_rate():\n",
    "    expo = 7\n",
    "    r = 1200\n",
    "    c = 10\n",
    "    for lr in (0.1, 0.01, 0.001, 0.0001, 0.00001):\n",
    "        j, s, l = load_train_datasets_by_res_and_coarseness(r, c)\n",
    "        X = np.concatenate((j[0], s[0], l[0]))\n",
    "        y = np.concatenate((j[1], s[1], l[1]))\n",
    "        name = f\"network_{infer_model_name(expo)}_res_{r}_coarse_{c}_lr_{lr}\"\n",
    "        train_model(X, y, name, expo=expo, dropout=_DROPOUT, lr=_LR, save=True)\n",
    "\n",
    "\n",
    "def make_model(*, input_shape=_INPUT_SHAPE, expo=_EXPO, dropout=_DROPOUT, lr=_LR):\n",
    "    def conv_block(\n",
    "        name, filters, kernel_size=4, pad=\"same\", t=False, act=\"relu\", bn=True\n",
    "    ):\n",
    "        \"\"\"High-level helper function to generate our block of convolutions.\n",
    "\n",
    "        Args:\n",
    "            name (str): The name of the block\n",
    "            filters (int): The number of filters to apply.\n",
    "            kernel_size (int, optional): The kernel size. Defaults to 4.\n",
    "            pad (str, optional): The type of padding to use. Defaults to \"same\".\n",
    "            t (bool, optional): Whether or not to transpose. Defaults to False.\n",
    "            act (str, optional): The activation function to use. Defaults to \"relu\".\n",
    "            bn (bool, optional): Whether or not to use batch norm. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            Sequential: The sequential block.\n",
    "        \"\"\"\n",
    "        block = Sequential(name=name)\n",
    "\n",
    "        if act == \"relu\":\n",
    "            block.add(L.ReLU())\n",
    "        elif act == \"leaky_relu\":\n",
    "            block.add(L.LeakyReLU(0.2))\n",
    "\n",
    "        if not t:\n",
    "            block.add(\n",
    "                L.Conv2D(\n",
    "                    filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=(2, 2),\n",
    "                    padding=pad,\n",
    "                    use_bias=True,\n",
    "                    activation=None,\n",
    "                    kernel_initializer=RandomNormal(0.0, 0.2),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            block.add(L.UpSampling2D(interpolation=\"bilinear\"))\n",
    "            block.add(\n",
    "                L.Conv2DTranspose(\n",
    "                    filters=filters,\n",
    "                    kernel_size=kernel_size - 1,\n",
    "                    padding=pad,\n",
    "                    activation=None,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if dropout > 0:\n",
    "            block.add(L.SpatialDropout2D(dropout))\n",
    "\n",
    "        if bn:\n",
    "            block.add(L.BatchNormalization(axis=-1, epsilon=1e-05, momentum=0.9))\n",
    "\n",
    "        return block\n",
    "\n",
    "    channels = int(2**expo + 0.5)\n",
    "    e0 = Sequential(name=\"enc_0\")\n",
    "    e0.add(\n",
    "        L.Conv2D(\n",
    "            filters=_CHANNELS,\n",
    "            kernel_size=4,\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=None,\n",
    "            data_format=\"channels_last\",\n",
    "        )\n",
    "    )\n",
    "    e1 = conv_block(\"enc_1\", channels, act=\"leaky_relu\")\n",
    "    e2 = conv_block(\"enc_2\", channels * 2, act=\"leaky_relu\")\n",
    "    e3 = conv_block(\"enc_3\", channels * 4, act=\"leaky_relu\")\n",
    "    e4 = conv_block(\"enc_4\", channels * 8, act=\"leaky_relu\", kernel_size=2, pad=\"valid\")\n",
    "    e5 = conv_block(\"enc_5\", channels * 8, act=\"leaky_relu\", kernel_size=2, pad=\"valid\")\n",
    "\n",
    "    dec_5 = conv_block(\"dec_5\", channels * 8, t=True, kernel_size=2, pad=\"valid\")\n",
    "    dec_4 = conv_block(\"dec_4\", channels * 8, t=True, kernel_size=2, pad=\"valid\")\n",
    "    dec_3 = conv_block(\"dec_3\", channels * 4, t=True)\n",
    "    dec_2 = conv_block(\"dec_2\", channels * 2, t=True)\n",
    "    dec_1 = conv_block(\"dec_1\", channels, act=None, t=True, bn=False)\n",
    "    dec_0 = Sequential(name=\"dec_0\")\n",
    "    dec_0.add(L.ReLU())\n",
    "    dec_0.add(L.Conv2DTranspose(_CHANNELS, kernel_size=4, strides=(2, 2), padding=\"same\"))\n",
    "\n",
    "    # Forward Pass\n",
    "    inputs = Input(shape=input_shape)\n",
    "    out0 = e0(inputs)\n",
    "    out1 = e1(out0)\n",
    "    out2 = e2(out1)\n",
    "    out3 = e3(out2)\n",
    "    out4 = e4(out3)\n",
    "    out5 = e5(out4)\n",
    "\n",
    "    dout5 = dec_5(out5)\n",
    "    dout5_out4 = tf.concat([dout5, out4], axis=3)\n",
    "    dout4 = dec_4(dout5_out4)\n",
    "    dout4_out3 = tf.concat([dout4, out3], axis=3)\n",
    "    dout3 = dec_3(dout4_out3)\n",
    "    dout3_out2 = tf.concat([dout3, out2], axis=3)\n",
    "    dout2 = dec_2(dout3_out2)\n",
    "    dout2_out1 = tf.concat([dout2, out1], axis=3)\n",
    "    dout1 = dec_1(dout2_out1)\n",
    "    dout1_out0 = tf.concat([dout1, out0], axis=3)\n",
    "    dout0 = dec_0(dout1_out0)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=dout0)\n",
    "    model.compile(optimizer=Adam(lr, beta_1=0.5), loss=_LOSS)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes the results of the model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def splice_label(name: str):\n",
    "    fname_split = name.split(\"_\")\n",
    "    label = \".\".join(\n",
    "        fname_split[fname_split.index(\"network\") + 1 : fname_split.index(\"res\")]\n",
    "    )\n",
    "    return label\n",
    "\n",
    "\n",
    "def plot_loss(histories: List[Tuple[str, History]], title: str, val=\"val_loss\"):\n",
    "    for label, history in histories:\n",
    "        hd = history.history\n",
    "        losses = hd[val]\n",
    "        epochs = range(1, len(losses) + 1)\n",
    "        plt.plot(epochs, losses, label=label)\n",
    "\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Epochs\", fontsize=16)\n",
    "    plt.ylabel(\"Loss\", fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def make_label_and_history_for_loss_comparison_plot(histories: List[str]):\n",
    "    def sort_by_lr(ls: List[str]) -> List[str]:\n",
    "        ret = [None] * 4\n",
    "        for f in ls:\n",
    "            if \"lr_0.01\" in f:\n",
    "                ret[0] = f\n",
    "            if \"lr_0.001\" in f:\n",
    "                ret[1] = f\n",
    "            if \"lr_0.0001\" in f:\n",
    "                ret[2] = f\n",
    "            if \"lr_1e-05\" in f:\n",
    "                ret[3] = f\n",
    "        return ret\n",
    "\n",
    "    def make_label_and_plot_data(files: List[str]):\n",
    "        for i, f in enumerate(files):\n",
    "            res = f[f.index(\"lr\") : f.index(\"scaled\")]\n",
    "            label = \" \".join(res.split(\"_\"))\n",
    "            files[i] = (label, pickle.load(open(f, \"rb\")))\n",
    "\n",
    "    histories = sort_by_lr(histories)\n",
    "    make_label_and_plot_data(histories)\n",
    "    return histories\n",
    "\n",
    "\n",
    "def make_label_and_history_for_model_size_comparison_plot(histories: List[str]):\n",
    "    def make_label_and_plot_data(files: List[str]):\n",
    "        for i, f in enumerate(files):\n",
    "            label = splice_label(f)\n",
    "            files[i] = (label, pickle.load(open(f, \"rb\")))\n",
    "\n",
    "    def sort_by_size(files: Tuple[str, Any]):\n",
    "        _map = {\n",
    "            \"21m\": 0,\n",
    "            \"5.4m\": 1,\n",
    "            \"1.3m\": 2,\n",
    "            \"330k\": 3,\n",
    "            \"86k\": 4,\n",
    "        }\n",
    "        ret = [None] * 5\n",
    "        for f in files:\n",
    "            ret[_map[f[0]]] = f\n",
    "        return ret\n",
    "\n",
    "    make_label_and_plot_data(histories)\n",
    "    histories = sort_by_size(histories)\n",
    "    return histories\n",
    "\n",
    "\n",
    "def make_label_and_history_for_coarseness_comparison(histories: List[str], c: int):\n",
    "    def make_label_and_plot_data(files: List[str]):\n",
    "        for i, f in enumerate(files):\n",
    "            res = \" \".join(f[f.index(\"res_\") : f.index(\"coarse_\") - 1].split(\"_\"))\n",
    "            files[i] = (res.capitalize(), pickle.load(open(f, \"rb\")))\n",
    "\n",
    "    def sort_by_resolution(files: List[str]) -> List[str]:\n",
    "        ret = [None] * 3\n",
    "        for f in files:\n",
    "            if \"400\" in f:\n",
    "                ret[0] = f\n",
    "            elif \"800\" in f:\n",
    "                ret[1] = f\n",
    "            elif \"1200\" in f:\n",
    "                ret[2] = f\n",
    "        return ret\n",
    "\n",
    "    histories = sort_by_resolution(list(filter(lambda x: f\"coarse_{c}\" in x, histories)))\n",
    "    make_label_and_plot_data(histories)\n",
    "    return histories\n",
    "\n",
    "\n",
    "def plot_validation_loss_by_learning_rate():\n",
    "    histories = list(\n",
    "        filter(\n",
    "            lambda x: x.startswith(\"history_\")\n",
    "            and \"21m\" in x\n",
    "            and \"coarse_10\" in x\n",
    "            and \"res_1200\" in x,\n",
    "            os.listdir(\".\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plt.ylim(0.0, 0.0040)\n",
    "    plot_loss(\n",
    "        make_label_and_history_for_loss_comparison_plot(histories),\n",
    "        \"Validation Loss by Learning Rate\",\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_validation_loss_by_model_size():\n",
    "    histories = list(\n",
    "        filter(\n",
    "            lambda x: x.startswith(\"history_\")\n",
    "            and \"lr_0.0001\" in x\n",
    "            and \"coarse_10\" in x\n",
    "            and \"res_1200\" in x,\n",
    "            os.listdir(\".\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plt.ylim(0, 0.0004)\n",
    "    plot_loss(\n",
    "        make_label_and_history_for_model_size_comparison_plot(histories),\n",
    "        \"Validation Loss for All Model Sizes\",\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_validation_loss_by_coarseness(c: int):\n",
    "    histories = list(\n",
    "        filter(\n",
    "            lambda x: x.startswith(\"history_\") and \"lr_0.0001\" in x and \"21m\" in x,\n",
    "            os.listdir(\".\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plot_loss(\n",
    "        make_label_and_history_for_coarseness_comparison(histories, c),\n",
    "        f\"Validation Loss for Coarseness {c}\",\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_sbs(c: int, dataset: np.ndarray, models: List[Model]):\n",
    "    steps_per_iter = _TEST_DATA_STEPS // c\n",
    "\n",
    "    def convert_x_label(params: int):\n",
    "        if params == 86262:\n",
    "            label = \"86k\"\n",
    "        elif params == 339014:\n",
    "            label = \"330k\"\n",
    "        elif params == 1_344_870:\n",
    "            label = \"1.3m\"\n",
    "        elif params == 5_357_990:\n",
    "            label = \"5.4m\"\n",
    "        elif params == 21_389_862:\n",
    "            label = \"21m\"\n",
    "        return label + \" params\"\n",
    "\n",
    "    fig, axes = plt.subplots(len(models) + 1, 3, figsize=(30, 30))\n",
    "    plt.subplots_adjust(bottom=0.2, top=0.8, right=0.65, left=0.35)\n",
    "    slice_ = steps_per_iter // 2\n",
    "    for i, axislist in enumerate(axes):\n",
    "        if i == 0:\n",
    "            prediction = dataset[1][slice_]\n",
    "            xlabel = \"Ground Truth\"\n",
    "        else:\n",
    "            model = models[i - 1]\n",
    "            xlabel = convert_x_label(model.count_params())\n",
    "            prediction = model.predict(np.expand_dims(dataset[1][slice_], axis=0))\n",
    "            prediction = np.squeeze(prediction)\n",
    "        for j, axis in enumerate(axislist):\n",
    "            axis.set_xticks([])\n",
    "            axis.set_yticks([])\n",
    "            axis.set_ylabel(_Y_AXIS_LABELS_GROUND_TRUTH[j], fontsize=14)\n",
    "            axis.set_xlabel(xlabel, fontsize=14)\n",
    "\n",
    "            im = axis.imshow(prediction[:, :, j], cmap=_CMAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error calculation\n",
    "\"\"\"\n",
    "\n",
    "def compute_cumulative_error(model: Model):\n",
    "    loss = Huber()\n",
    "    dataset = np.load(\"dataset_res_1500_gmin_-100_to_gmax_-99_coarse_10_all.npz\")\n",
    "\n",
    "    dataset = make_inputs_and_targets(dataset)\n",
    "    transform(dataset)\n",
    "\n",
    "    dsini = dataset[0]\n",
    "    dsfin = dataset[1]\n",
    "\n",
    "    cum_error = 0.0\n",
    "    for inp, gt in zip(dsini, dsfin):\n",
    "        ypred = np.squeeze(model.predict(np.expand_dims(inp, axis=0)))\n",
    "        cum_error += loss(gt * 140, ypred * 140).numpy()\n",
    "\n",
    "    return cum_error / dsini.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Sep 13 2022, 22:03:16) \n[Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
